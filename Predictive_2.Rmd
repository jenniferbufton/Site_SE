---
title: Predictive Modelling
output: 
  html_document:
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: flatly
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(dplyr)
library(readxl)
library(rpart)
library(rpart.plot)
library(kableExtra)
library(class)
library(gmodels)
library(rlang)
library(ggplot2)
library(extrafont)

setwd("C:/Users/jenniferb/OneDrive - Sport England/GitHub/Small Grants")
me_df <- read_xlsx('MandE.xlsx', na = "NA")

se_colour <- c( "#e41b4a", "#0072d6", "#a4569c", "#00a881", "#ff6105") # custom colours to use in the chart



```

# K-Nearest Neighbours

## Introduction

K-Nearest neighbours (or KNN), is a way of classifying a new observation based on already existing observations that have previously been classified. It is 'black box' - we don't know the rules to understand the relationships between features and the classifications.

We can use this algorithm to see if we can predict if a project will exceed its target. In this instance, we are using 8 predictor variables, and the target variable is "Over Target" (True or False).

We can split the dataset into a Test dataset and a Training dataset to predict the outcome variable, and then evaluate those predictions. 

```{r knn, echo=FALSE,  fig.align="center",}
data <- me_df%>%
  mutate("Over_Target" = `12 MONTH PARTICIPANTS` > `YEAR 1 PARTICIPANTS TARGET`)%>%
  select(AWARD, Total_Project_Cost, Partnership_Funding, IMD_RANK, `PARTICIPANTS BASELINE`, `YEAR 1 PARTICIPANTS TARGET`,
         `FEMALE TARGET PERCENTAGE`, `MALE TARGET PERCENTAGE`, Over_Target)
  

data$IMD_RANK <-  as.numeric(data$IMD_RANK)

d <- lapply(data[,-9], scale)
d <- as.data.frame(d)
d$Over_Target <- data$Over_Target  

d <- na.omit(d)

a<- d %>%
  rename("Award" = AWARD, `Total Cost` = Total_Project_Cost, `Partnership Funding` = Partnership_Funding, `IMD Rank`= IMD_RANK, "Baseline" = PARTICIPANTS.BASELINE, `12 Month Target` = YEAR.1.PARTICIPANTS.TARGET, `Female Target` = FEMALE.TARGET.PERCENTAGE, `Male Target` = MALE.TARGET.PERCENTAGE, `Over Target - Target Variable` = Over_Target )

kable(colnames(a), col.names = "Variables Used")%>%
  kable_styling(bootstrap_options = "bordered",
                full_width = FALSE) 


```

## Classifiying the Data

For the unclassified observations, the algorithm measures the K closest neighbours based on distance (the value of K in this instance in the square root of the number of rows in the dataframe). The algorithm selects which class is the most common in the training dataset, and predicts those new observations accordingly

The below table shows the accuracy of the KNN algorithm when classifying a test dataset. It classified 60% correctly, and 40% incorrectly:

```{r knn2, fig.align="center", message=FALSE, warning=FALSE,  echo=FALSE,}

#sampling method 2, using data partition function from caret package. It attempts to retain similar distributions of species across the train and test datasets. Times argument is number of partitions to create , List defines how you want the multiple partitions stored (list or matrix) 
# more info on this method here https://topepo.github.io/caret/data-splitting.html
set.seed(123)
train_index <- createDataPartition(d$Over_Target, p = 0.8, times=1, list = FALSE) # from caret package. !st argument is dependent varaible, p = split, times = number it runs, list = false if 1 tmes

train_d <- d[train_index,]
test_d <- d[-train_index,]

# testing distribution
#table((d$Over_Target))
#prop.table(table(d$Over_Target))
#prop.table(table(train_d$Over_Target))
#prop.table(table(test_d$Over_Target))

#knn
train_d_no_class <- train_d[,-9] # no target variable (Over_Target in this case)
test_d_no_class <- test_d[,-9] # no target variable (Over_Target in this case)
train_d_labels <- train_d[,9] # Only target variable (Over_Target in this case)

# sqrt(nrow(d)) # find square root of nrows 

# 8 predictors in this model
d_test_pred <- knn(train = train_d_no_class, test = test_d_no_class,
                   cl = train_d_labels, k = 27)

#evaluation
test_d$pred <- d_test_pred # add new prediction column

test_d$correct <- 'Incorrect'
test_d$correct[test_d$Over_Target == test_d$pred] <- 'Correct' # Add another column which shows if correct

#kable(table(test_d$correct)) 
kable(prop.table(table(test_d$correct)), col.names = c("Correct?","Proportion"))%>%
  kable_styling(bootstrap_options = "bordered",
                full_width = FALSE) 

```
## Evaluating the Classifications
We can see more detail around the classifications using the `CrossTable` function. As can be seen in the table below, the main issue with the predictions were that 45 of those that were "True" (i.e. were over the 12 month target),  were  predicted "False".

Only 11% (9 cases) of those that were FALSE were misclassified as TRUE.


```{r knn3, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE }
CrossTable(x = test_d$Over_Target, y = d_test_pred,
          prop.chisq=FALSE)
```

The 60% accuracy is problematic for applying this to new applications as it currently stands.

One potential way to improve the predictions would be to add other predictor variables (e.g. including target numbers around age, ethnicity, and disability). 


# Decision Trees

The ‘tree’ is a sequence of decisions (branches) that aim to maximise the reduction in the variation in the outcome variable (in this case, the number of participants at 12 months).

<br>

# Outputs  {.tabset .tabset-fade .tabset-pills} 
The model below shows the first "branch", i.e. the most important variable out of those included that explain variation.  This is the "Total Project Cost" - if the total cost is greater than or equal to £23,707.93 (4% of the awards), the mean number of participants is 611 . Less than this amount, and the mean number is 176, and this accounts for 96% of the awards.

The model was limited to the most important variable (branch 1). The more depth we go down, the better the model will 'fit' our data. This can be problematic if we tried to predict an outcome based on this model. The data may become 'over-fitted' (i.e. the model is only acccurate when applied to this specific dataset).

## Decision Tree Plot
```{r tree.1, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}

data <- me_df%>%
  select(AWARD, Total_Project_Cost, Partnership_Funding, GOVERNMENT_OFFICE_REGION, Focus,
         ALLOCATED_TO, IMD_RANK, `12 MONTH PARTICIPANTS`, Org_Class)

data <- as.data.frame(data)
data$IMD_RANK <- as.numeric(data$IMD_RANK)
data$GOVERNMENT_OFFICE_REGION <- as.factor(data$GOVERNMENT_OFFICE_REGION)
data$Focus <- as.factor(data$Focus)
data$ALLOCATED_TO <- as.factor(data$ALLOCATED_TO)

#set.seed(123)
inTrain <- createDataPartition(y=data$`12 MONTH PARTICIPANTS`, p=1, list=FALSE) 
Train <- data[inTrain,]
#Test <- data[-inTrain,]

tree <- rpart(`12 MONTH PARTICIPANTS` ~., data=Train, maxdepth = 1)
rpart.plot(tree, tweak = 1.5, cex = 0.5, type = 1)
```

## Decision Tree Summary
```{r tree.2, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
tree
```


## Variable Importance
```{r tree.3, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
tree$variable.importance
```

